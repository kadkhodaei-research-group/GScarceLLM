{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODAkFQQ2Dl90",
        "outputId": "b4a3f49a-16e8-43d1-c50c-6403edc3febd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final dataset size after processing: 164\n",
            "Train JSONL file has been created: train_dataset.jsonl\n",
            "Validation JSONL file has been created: validation_dataset.jsonl\n",
            "System prompt used: Dynamic\n",
            "Classification type used: Ternary\n",
            "Dataset type used: General\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd # type: ignore\n",
        "import json\n",
        "import random\n",
        "\n",
        "# Set the dataset type and other configurations\n",
        "DATASET_TYPE = \"general\"\n",
        "USE_DYNAMIC_PROMPT = True\n",
        "N_CLASSES = 3  # Set to 2 for binary classification, 3 for ternary classification\n",
        "\n",
        "# Load the dataset based on type\n",
        "if DATASET_TYPE == \"general\":\n",
        "    csv_file = \"KNN_final_imputed_data.csv\"\n",
        "elif DATASET_TYPE == \"numerical\":\n",
        "    csv_file = \"Binary_GIST_SE_8_DT.csv\"\n",
        "\n",
        "data = pd.read_csv(csv_file, encoding=\"utf-8\")\n",
        "\n",
        "data.columns = [col.replace(\"\\u00b0C\", \"°C\") for col in data.columns]\n",
        "\n",
        "# Define the target column and attribute columns\n",
        "target_column = \"No. of Graphene Layers\"\n",
        "attribute_columns = [col for col in data.columns if col != target_column]\n",
        "\n",
        "# Define units for attributes (for general datasets only)\n",
        "units = {\n",
        "    \"Pressure (mbar)\": \"mbar\",\n",
        "    \"Temperature (°C)\": \"°C\",\n",
        "    \"Growth Time (min)\": \"min\",\n",
        "    \"H2\": \"sccm\",\n",
        "    \"CH4\": \"sccm\",\n",
        "    \"C2H4\": \"sccm\",\n",
        "    \"Ar\": \"sccm\",\n",
        "    \"C2H2\": \"sccm\",\n",
        "}\n",
        "\n",
        "if DATASET_TYPE == \"general\":\n",
        "    # Function to encode graphene layers\n",
        "    def encode_graphene_layers(data, n_classes=2):\n",
        "        \"\"\"Encode 'No. of Graphene Layers' as a categorical variable.\"\"\"\n",
        "        data[target_column] = data[target_column].replace(['ML', 'Unknown'], 10.0)\n",
        "        data[target_column] = pd.to_numeric(data[target_column], errors='coerce')\n",
        "        data[target_column] = data[target_column].fillna(0)\n",
        "\n",
        "        if n_classes == 2:\n",
        "            data[target_column] = data[target_column].apply(lambda x: 0 if x <= 1 else 1)\n",
        "        elif n_classes == 3:\n",
        "            data[target_column] = data[target_column].apply(lambda x: 0 if x < 1.5\n",
        "                                                             else 1 if 1.5 <= x < 2.5\n",
        "                                                             else 2)\n",
        "        elif n_classes == 4:\n",
        "            data[target_column] = data[target_column].apply(lambda x: 0 if x < 1.5\n",
        "                                                             else 1 if 1.5 <= x < 2.5\n",
        "                                                             else 2 if 2.5 <= x < 3.5\n",
        "                                                             else 3)\n",
        "\n",
        "        data[target_column] = data[target_column].astype('category')\n",
        "        return data\n",
        "\n",
        "    # Apply encoding for general datasets\n",
        "    data = encode_graphene_layers(data, n_classes=N_CLASSES)\n",
        "\n",
        "print(f\"Final dataset size after processing: {len(data)}\")\n",
        "\n",
        "jsonl_data = []\n",
        "\n",
        "for _, row in data.iterrows():\n",
        "    if DATASET_TYPE == \"general\":\n",
        "        # User prompt with units\n",
        "        attributes_description = [\n",
        "            f\"{col} is {row[col]} {units[col]}\" if col in units else f\"{col} is {row[col]}\"\n",
        "            for col in attribute_columns\n",
        "        ]\n",
        "    else:\n",
        "        # User prompt without units (numerical dataset)\n",
        "        attributes_description = [f\"{col} is {row[col]}\" for col in attribute_columns]\n",
        "\n",
        "    user_prompt = \", \".join(attributes_description)\n",
        "\n",
        "    # Assistant response: description of the target\n",
        "    assistant_response = f\"{row[target_column]}.\"\n",
        "\n",
        "    # Select system prompt based on dataset type\n",
        "    if DATASET_TYPE == \"general\":\n",
        "        system_prompt = (\n",
        "            \"You are an expert in Chemical Vapor Deposition (CVD) of Graphene with a strong \"\n",
        "            \"understanding of process variables. Your task is to predict the number of graphene layers \"\n",
        "            \"based on the following attributes: \"\n",
        "            + \", \".join(attribute_columns) +\n",
        "            \". The target variable to predict is: \" + target_column + \".\"\n",
        "        ) if USE_DYNAMIC_PROMPT else (\n",
        "            \"You are an expert in Chemical Vapor Deposition (CVD) of Graphene. \"\n",
        "            \"Your task is to predict the number of graphene layers based on process variables.\"\n",
        "        )\n",
        "    else:  # Numerical dataset system prompt\n",
        "        system_prompt = (\n",
        "            \"You are an expert in Chemical Vapor Deposition (CVD) of Graphene with a strong \"\n",
        "            \"understanding of process variables. Your task is to predict the number of graphene layers \"\n",
        "            \"based on the following attributes: \"\n",
        "            + \", \".join(attribute_columns) +\n",
        "            \". The target variable to predict is: \" + target_column + \".\"\n",
        "        )\n",
        "        '''system_prompt = (\n",
        "            \"You are an expert in data-driven prediction. The dataset consists of pre-processed, \"\n",
        "            \"feature-engineered, and normalized attributes extracted using advanced techniques for graphene CVD. \"\n",
        "            \"The values are unitless, transformed features. \"\n",
        "            \"Your task is to predict the number of graphene layers based on these featurized attributes.\"\n",
        "        )'''\n",
        "\n",
        "    # Append to JSONL format\n",
        "    jsonl_data.append({\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "            {\"role\": \"assistant\", \"content\": assistant_response}\n",
        "        ]\n",
        "    })\n",
        "\n",
        "# Shuffle the data for randomness\n",
        "random.shuffle(jsonl_data)\n",
        "\n",
        "# Split the data into train (80%) and validation (20%)\n",
        "split_index = int(0.8 * len(jsonl_data))\n",
        "train_data, validation_data = jsonl_data[:split_index], jsonl_data[split_index:]\n",
        "\n",
        "# Save train data to JSONL file\n",
        "train_gpt = \"train_dataset.jsonl\"\n",
        "with open(train_gpt, \"w\", encoding=\"utf-8\") as f:\n",
        "    for entry in train_data:\n",
        "        f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "# Save validation data to JSONL file\n",
        "validation_gpt = \"validation_dataset.jsonl\"\n",
        "with open(validation_gpt, \"w\", encoding=\"utf-8\") as f:\n",
        "    for entry in validation_data:\n",
        "        f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(f\"Train JSONL file has been created: {train_gpt}\")\n",
        "print(f\"Validation JSONL file has been created: {validation_gpt}\")\n",
        "print(f\"System prompt used: {'Dynamic' if USE_DYNAMIC_PROMPT else 'Constant'}\")\n",
        "print(f\"Classification type used: {'Ternary' if N_CLASSES == 3 else 'Binary'}\")\n",
        "print(f\"Dataset type used: {DATASET_TYPE.capitalize()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifm2M_e5RgSz",
        "outputId": "ae2e0c9d-8074-4e41-b8d6-0e12d806ff65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.61.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNZZ-46XRmkR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \" \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ew3bnMj9QtBN",
        "outputId": "ffb482bf-1377-470d-b2b9-bb7fcf8b9088"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['0.', '2.', '0.', '0.', '0.', '0.', '0.', '0.', '0.', '0.', '0.', '0.', '0.', '0.', '0.', '0.', '0.', '0.', '0.', '0.', '0.', '0.', '0.', '0.', '0.', '0.', '0.', '0.', '0.', '0.', '0.', '0.', '0.']\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import openai\n",
        "\n",
        "# Initialize the OpenAI client\n",
        "client = openai.OpenAI()\n",
        "\n",
        "# Path to your JSONL file\n",
        "jsonl_file_path = \"validation_dataset.jsonl\"\n",
        "\n",
        "# List to store model responses\n",
        "responses = []\n",
        "\n",
        "# Read the JSONL file and process each entry\n",
        "with open(jsonl_file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    for line in file:\n",
        "        data = json.loads(line)  # Load each JSON object\n",
        "\n",
        "        # Extract messages\n",
        "        messages = data[\"messages\"]\n",
        "\n",
        "        # Filter only the system and user messages\n",
        "        filtered_messages = [\n",
        "            msg for msg in messages if msg[\"role\"] in [\"system\", \"user\"]\n",
        "        ]\n",
        "\n",
        "        # Send request to the fine-tuned model\n",
        "        try:\n",
        "            completion = client.chat.completions.create(\n",
        "                model=\"ft:gpt-4o-mini-2024-07-18:cmrlresearchlab:knn-grternarylayers:B0foOlYg\",\n",
        "                messages=filtered_messages\n",
        "            )\n",
        "\n",
        "            # Extract assistant's response\n",
        "            response = completion.choices[0].message.content\n",
        "            responses.append(response)\n",
        "\n",
        "        except openai.OpenAIError as e:\n",
        "            print(f\"Error processing entry: {e}\")\n",
        "\n",
        "# Print all collected responses\n",
        "print(responses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# User-defined choice: \"binary\" or \"ternary\"\n",
        "classification_type = input(\"Enter classification type (binary/ternary): \").strip().lower()\n",
        "\n",
        "# Validate input\n",
        "if classification_type not in [\"binary\", \"ternary\"]:\n",
        "    raise ValueError(\"Invalid input! Please enter 'binary' or 'ternary'.\")\n",
        "\n",
        "# Lists to store true labels\n",
        "true_labels = []\n",
        "\n",
        "# Read the JSONL file and extract true labels\n",
        "with open(jsonl_file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    for line in file:\n",
        "        data = json.loads(line)  # Load each JSON object\n",
        "\n",
        "        # Extract the true label (assistant's response)\n",
        "        true_label = next((msg[\"content\"] for msg in data[\"messages\"] if msg[\"role\"] == \"assistant\"), None)\n",
        "\n",
        "        # Convert true label to an integer (handle \"0.\", \"1.\", \"2.\")\n",
        "        if true_label is not None:\n",
        "            true_labels.append(int(true_label.strip().replace(\".\", \"\")))  # Convert \"2.\" -> 2, \"1.\" -> 1, \"0.\" -> 0\n",
        "\n",
        "# Ensure predicted labels are also integers\n",
        "predicted_labels = [int(pred.strip().replace(\".\", \"\")) for pred in responses]\n",
        "\n",
        "# Compute evaluation metrics based on classification type\n",
        "if classification_type == \"binary\":\n",
        "    average_method = \"binary\"\n",
        "else:  # Ternary classification\n",
        "    average_method = \"macro\"\n",
        "\n",
        "# Calculate performance metrics\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "precision = precision_score(true_labels, predicted_labels, average=average_method)\n",
        "recall = recall_score(true_labels, predicted_labels, average=average_method)\n",
        "f1 = f1_score(true_labels, predicted_labels, average=average_method)\n",
        "report = classification_report(true_labels, predicted_labels)\n",
        "\n",
        "# Print results\n",
        "print(f\"\\nClassification Type: {classification_type.capitalize()}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(\"\\nClassification Report:\\n\", report)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
